{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4108d9cf",
   "metadata": {},
   "source": [
    "# **LCOFI Algorithm on Pakistan Accident Dataset**\n",
    "\n",
    "This document explains the process of applying the **LCOFI (Logic Circuit Optimization Frequent Itemset)** algorithm to a dataset of Pakistan traffic accidents. The algorithm mines frequent itemsets and generates association rules to uncover meaningful patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **Steps in the Code**\n",
    "\n",
    "### **1. Preprocessing the Dataset**\n",
    "- The dataset, stored in a CSV file, contains columns such as `Area`, `Year`, `Total number of accidents`, etc.\n",
    "- Each row in the dataset is converted into a transaction where each column value is represented as `ColumnName_Value`. For example:\n",
    "  - `Area_Punjab`, `Year_2021`, `Killed_10`.\n",
    "- Transactions are stored as sets for efficient processing.\n",
    "\n",
    "### **2. Define the LCOFI Algorithm**\n",
    "- **Purpose**: Mine frequent itemsets using a graph-based approach.\n",
    "- **Key Components**:\n",
    "  1. **Bipartite Graph Representation**:\n",
    "     - Transactions and items are represented as nodes in a bipartite graph.\n",
    "     - Edges connect transactions to their items.\n",
    "  2. **Frequent Itemset Mining**:\n",
    "     - The algorithm starts with single-itemsets and iteratively generates larger itemsets.\n",
    "     - Support for itemsets is counted by checking their presence in transactions.\n",
    "     - Itemsets that meet the minimum support threshold are retained, while others are pruned.\n",
    "\n",
    "### **3. Generate Association Rules**\n",
    "- **Purpose**: Extract meaningful relationships between items in the frequent itemsets.\n",
    "- **Process**:\n",
    "  1. Flatten the frequent itemsets into a single dictionary.\n",
    "  2. Prepare a DataFrame containing the itemsets and their normalized support values.\n",
    "  3. Use the `mlxtend.frequent_patterns.association_rules` function to compute rules.\n",
    "     - Metrics include:\n",
    "       - **Support**: Proportion of transactions containing the itemset.\n",
    "       - **Confidence**: Likelihood that the consequent occurs given the antecedent.\n",
    "       - **Lift**: Measure of the strength of the rule compared to random chance.\n",
    "\n",
    "### **4. Load and Analyze the Dataset**\n",
    "- **Input File**: `pak-traffic-accidents-annual.csv`.\n",
    "- **Parameters**:\n",
    "  - `min_support`: Set to 0.09, meaning an itemset must appear in at least 9% of the transactions.\n",
    "  - `min_confidence`: Set to 0.6, meaning rules must have at least 60% confidence.\n",
    "- The dataset is preprocessed into transactions, and the LCOFI algorithm is run to mine frequent itemsets.\n",
    "- Association rules are then generated and displayed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Outputs**\n",
    "1. **Frequent Itemsets**:\n",
    "   - Lists itemsets at each level (e.g., single items, two-item combinations) with their support values.\n",
    "   - Example:\n",
    "     ```\n",
    "     Level 1:\n",
    "       {'Area_Punjab'}: 0.50\n",
    "       {'Year_2021'}: 0.40\n",
    "     Level 2:\n",
    "       {'Area_Punjab', 'Year_2021'}: 0.30\n",
    "     ```\n",
    "\n",
    "2. **Association Rules**:\n",
    "   - Displays relationships between itemsets with metrics such as support, confidence, and lift.\n",
    "   - Example:\n",
    "     ```\n",
    "     {'Area_Punjab'} => {'Year_2021'} (Support: 0.30, Confidence: 0.60, Lift: 1.20)\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Benefits of This Implementation**\n",
    "1. **Efficient Mining**:\n",
    "   - The LCOFI algorithm uses a graph-based approach to minimize dataset scans and computational overhead.\n",
    "2. **Customizable Thresholds**:\n",
    "   - Parameters like `min_support` and `min_confidence` can be adjusted to control the level of detail in the output.\n",
    "3. **Actionable Insights**:\n",
    "   - Association rules provide valuable patterns that can guide decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications**\n",
    "- **Traffic Safety**:\n",
    "  - Identify patterns in accident data (e.g., frequent occurrences in certain areas or time periods).\n",
    "- **Policy Making**:\n",
    "  - Use insights to target interventions, such as improving road safety in high-risk areas.\n",
    "- **Healthcare**:\n",
    "  - Analyze injury patterns to enhance emergency response systems.\n",
    "\n",
    "This code demonstrates a practical approach to frequent itemset mining and association rule generation using real-world data. Adjust parameters as needed to tailor the analysis for different datasets or objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "Level 1:\n",
      "  {'Area_Khyber Pakhtunkhwa'}: 0.18\n",
      "  {'Area_Punjab'}: 0.18\n",
      "  {'Area_Pakistan'}: 0.18\n",
      "  {'Year_2017-18'}: 0.10\n",
      "  {'Area_Islamabad'}: 0.11\n",
      "  {'Year_2018-19'}: 0.10\n",
      "  {'Area_Sindh'}: 0.18\n",
      "  {'Area_Balochistan'}: 0.18\n",
      "\n",
      "Association Rules:\n",
      "Empty DataFrame\n",
      "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, representativity, leverage, conviction, zhangs_metric, jaccard, certainty, kulczynski]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Step 1: Preprocess the Dataset\n",
    "def preprocess_accident_data(filename):\n",
    "    \"\"\"Preprocess the Pakistan accident dataset into transactions.\"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    # Convert each row into a transaction\n",
    "    transactions = []\n",
    "    for _, row in data.iterrows():\n",
    "        transaction = set()\n",
    "        for col in row.index:\n",
    "            if pd.notnull(row[col]):\n",
    "                transaction.add(f\"{col}_{row[col]}\")\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Step 2: Define LCOFI Algorithm\n",
    "def generate_candidates(frequent_itemsets, size):\n",
    "    \"\"\"Generate candidate itemsets of a specific size.\"\"\"\n",
    "    return set(\n",
    "        frozenset(x) for x in itertools.combinations(set(itertools.chain(*frequent_itemsets)), size)\n",
    "    )\n",
    "\n",
    "def count_support(itemsets, transactions, min_support):\n",
    "    \"\"\"Count the support of itemsets.\"\"\"\n",
    "    support_counts = {item: 0 for item in itemsets}\n",
    "    for transaction in transactions:\n",
    "        for item in itemsets:\n",
    "            if item.issubset(transaction):\n",
    "                support_counts[item] += 1\n",
    "    return {\n",
    "        item: count for item, count in support_counts.items()\n",
    "        if count / len(transactions) >= min_support\n",
    "    }\n",
    "\n",
    "def lcofi(transactions, min_support):\n",
    "    \"\"\"LCOFI algorithm for frequent itemsets using graph representation.\"\"\"\n",
    "    # Build a bipartite graph\n",
    "    G = nx.Graph()\n",
    "    for i, transaction in enumerate(transactions):\n",
    "        for item in transaction:\n",
    "            G.add_edge(f\"Transaction_{i}\", item)\n",
    "\n",
    "    # Generate single-item frequent itemsets\n",
    "    single_items = {frozenset([node]) for node in G if G.degree[node] > 0 and not node.startswith(\"Transaction\")}\n",
    "    frequent_itemsets = count_support(single_items, transactions, min_support)\n",
    "\n",
    "    all_frequent_itemsets = [frequent_itemsets]\n",
    "\n",
    "    # Iteratively generate larger itemsets\n",
    "    k = 2\n",
    "    while frequent_itemsets:\n",
    "        candidates = generate_candidates(frequent_itemsets, k)\n",
    "        frequent_itemsets = count_support(candidates, transactions, min_support)\n",
    "        if frequent_itemsets:\n",
    "            all_frequent_itemsets.append(frequent_itemsets)\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "# Step 3: Generate Association Rules\n",
    "def generate_association_rules(frequent_itemsets, transactions, min_confidence):\n",
    "    \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
    "    # Flatten frequent itemsets\n",
    "    flat_itemsets = {}\n",
    "    for level in frequent_itemsets:\n",
    "        flat_itemsets.update(level)\n",
    "\n",
    "    # Prepare DataFrame\n",
    "    num_transactions = len(transactions)\n",
    "    data = {\n",
    "        'itemsets': list(flat_itemsets.keys()),\n",
    "        'support': [support / num_transactions for support in flat_itemsets.values()]\n",
    "    }\n",
    "    frequent_itemsets_df = pd.DataFrame(data)\n",
    "\n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets_df, metric=\"confidence\", min_threshold=min_confidence, num_itemsets=num_transactions)\n",
    "    return rules\n",
    "\n",
    "# Step 4: Load and Analyze the Dataset\n",
    "filename = \"Datasets/pak-traffic-accidents-annual.csv\"\n",
    "transactions = preprocess_accident_data(filename)\n",
    "\n",
    "# Parameters\n",
    "min_support = 0.09  # Minimum support threshold\n",
    "min_confidence = 0.2  # Minimum confidence threshold\n",
    "\n",
    "# Run LCOFI Algorithm\n",
    "frequent_itemsets = lcofi(transactions, min_support)\n",
    "\n",
    "# Generate and Print Association Rules\n",
    "rules = generate_association_rules(frequent_itemsets, transactions, min_confidence)\n",
    "\n",
    "# Display Results\n",
    "print(\"Frequent Itemsets:\")\n",
    "for k, itemsets in enumerate(frequent_itemsets, start=1):\n",
    "    print(f\"Level {k}:\")\n",
    "    for itemset, support in itemsets.items():\n",
    "        print(f\"  {set(itemset)}: {support / len(transactions):.2f}\")\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a51fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
