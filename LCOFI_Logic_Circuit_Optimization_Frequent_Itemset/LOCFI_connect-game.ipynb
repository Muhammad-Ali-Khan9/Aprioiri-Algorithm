{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4108d9cf",
   "metadata": {},
   "source": [
    "# **LCOFI Algorithm on Connect Game Dataset**\n",
    "\n",
    "This document explains the logic and steps involved in applying the **LCOFI (Logic Circuit Optimization Frequent Itemset)** algorithm to the Connect Game dataset. The dataset consists of positional values (`-1`, `0`, `1`) for a Connect Game grid, and the algorithm uncovers frequent patterns and association rules.\n",
    "\n",
    "---\n",
    "\n",
    "## **Logic Workflow**\n",
    "\n",
    "### **1. Preprocessing the Dataset**\n",
    "- **Objective**: Convert the Connect Game dataset into a transactional format suitable for frequent itemset mining.\n",
    "- **Approach**:\n",
    "  - Each position in the grid (e.g., `pos_01`, `pos_02`, ...) is treated as an item.\n",
    "  - The value of the position (`-1`, `0`, `1`) is combined with the position name to create unique item labels (e.g., `pos_01_-1`).\n",
    "  - Each row in the dataset is transformed into a transaction consisting of these labels.\n",
    "\n",
    "### **2. LCOFI Algorithm**\n",
    "The LCOFI algorithm mines frequent itemsets using a **graph-based approach**:\n",
    "\n",
    "#### **2.1 Bipartite Graph Construction**\n",
    "- Transactions and items are represented as nodes in a bipartite graph.\n",
    "- Edges connect transaction nodes to their corresponding items.\n",
    "- This representation simplifies traversal and relationship analysis.\n",
    "\n",
    "#### **2.2 Frequent Itemset Generation**\n",
    "- The algorithm starts with single-itemsets and calculates their support.\n",
    "- Using the **Apriori property**, larger itemsets are iteratively generated by combining frequent subsets.\n",
    "- Support is recalculated for these larger itemsets, and infrequent ones are pruned.\n",
    "\n",
    "### **3. Association Rule Generation**\n",
    "- Association rules are derived from the mined frequent itemsets.\n",
    "- **Key Metrics**:\n",
    "  - **Support**: Fraction of transactions containing the itemset.\n",
    "  - **Confidence**: Likelihood of the consequent occurring, given the antecedent.\n",
    "  - **Lift**: Strength of the rule compared to random chance.\n",
    "- Rules are filtered based on a minimum confidence threshold to ensure significance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Dataset**\n",
    "The Connect Game dataset contains 42 positional columns (`pos_01` to `pos_42`) and a `winner` column:\n",
    "- Each position holds a value (`-1`, `0`, `1`) indicating its state.\n",
    "- Transactions are constructed using these positional values (e.g., `pos_01_-1`, `pos_02_1`).\n",
    "\n",
    "---\n",
    "\n",
    "## **Steps in the Workflow**\n",
    "\n",
    "1. **Input**:\n",
    "   - Dataset in CSV format with columns for positional values and the winner.\n",
    "   - Example: `pos_01=-1`, `pos_02=1`.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - Transform the dataset into transactions by combining column names with their values.\n",
    "   - Example Transaction: `{pos_01_-1, pos_02_1, pos_03_0}`.\n",
    "\n",
    "3. **Frequent Itemset Mining**:\n",
    "   - Use the bipartite graph representation to:\n",
    "     - Count the support for each itemset.\n",
    "     - Prune infrequent itemsets dynamically.\n",
    "     - Iteratively generate larger itemsets.\n",
    "\n",
    "4. **Association Rule Generation**:\n",
    "   - Derive rules from frequent itemsets.\n",
    "   - Filter rules based on minimum confidence and lift.\n",
    "\n",
    "---\n",
    "\n",
    "## **Expected Outputs**\n",
    "\n",
    "### **1. Frequent Itemsets**\n",
    "- Identifies itemsets that occur frequently in the dataset.\n",
    "- Example:\n",
    "  - Single-itemsets: `{pos_01_1}: 0.15`, `{pos_02_-1}: 0.12`.\n",
    "  - Two-itemsets: `{pos_01_1, pos_02_-1}: 0.10`.\n",
    "\n",
    "### **2. Association Rules**\n",
    "- Highlights relationships between itemsets.\n",
    "- Example:\n",
    "  - Rule: `{pos_01_1} => {pos_02_-1}`\n",
    "    - Support: 0.10\n",
    "    - Confidence: 0.66\n",
    "    - Lift: 1.20\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Features of LCOFI**\n",
    "- **Efficient Representation**:\n",
    "  - Bipartite graph reduces memory usage and computation time.\n",
    "- **Dynamic Pruning**:\n",
    "  - Infrequent itemsets are removed during traversal, minimizing overhead.\n",
    "- **Iterative Expansion**:\n",
    "  - Larger itemsets are generated only from frequent subsets, ensuring relevance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications**\n",
    "- **Game Analysis**:\n",
    "  - Identify patterns in winning or losing moves.\n",
    "- **Pattern Recognition**:\n",
    "  - Discover positional combinations that frequently occur together.\n",
    "\n",
    "---\n",
    "\n",
    "## **Parameters**\n",
    "- **Minimum Support (`min_support`)**:\n",
    "  - Controls how often an itemset must appear to be considered frequent.\n",
    "- **Minimum Confidence (`min_confidence`)**:\n",
    "  - Ensures that generated rules are significant and actionable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "This implementation of the LCOFI algorithm provides an efficient and scalable way to mine frequent patterns and generate association rules from the Connect Game dataset. By leveraging a graph-based approach, the algorithm ensures optimal performance even for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf3823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "Level 1:\n",
      "  {'pos_07_0.0'}: 0.91\n",
      "  {'pos_08_0.0'}: 0.76\n",
      "  {'pos_14_0.0'}: 0.84\n",
      "  {'pos_05_0.0'}: 0.78\n",
      "  {'pos_02_0.0'}: 0.77\n",
      "  {'pos_13_0.0'}: 0.74\n",
      "  {'pos_06_0.0'}: 0.85\n",
      "  {'pos_01_0.0'}: 0.86\n",
      "  {'pos_21_0.0'}: 0.74\n",
      "Level 2:\n",
      "  {'pos_01_0.0', 'pos_07_0.0'}: 0.83\n",
      "  {'pos_02_0.0', 'pos_06_0.0'}: 0.70\n",
      "  {'pos_08_0.0', 'pos_07_0.0'}: 0.73\n",
      "  {'pos_06_0.0', 'pos_13_0.0'}: 0.74\n",
      "  {'pos_21_0.0', 'pos_07_0.0'}: 0.74\n",
      "  {'pos_05_0.0', 'pos_06_0.0'}: 0.71\n",
      "  {'pos_06_0.0', 'pos_14_0.0'}: 0.76\n",
      "  {'pos_21_0.0', 'pos_14_0.0'}: 0.74\n",
      "  {'pos_14_0.0', 'pos_07_0.0'}: 0.84\n",
      "  {'pos_05_0.0', 'pos_07_0.0'}: 0.75\n",
      "  {'pos_02_0.0', 'pos_07_0.0'}: 0.74\n",
      "  {'pos_01_0.0', 'pos_06_0.0'}: 0.78\n",
      "  {'pos_01_0.0', 'pos_08_0.0'}: 0.76\n",
      "  {'pos_05_0.0', 'pos_14_0.0'}: 0.70\n",
      "  {'pos_01_0.0', 'pos_02_0.0'}: 0.72\n",
      "  {'pos_13_0.0', 'pos_07_0.0'}: 0.71\n",
      "  {'pos_06_0.0', 'pos_07_0.0'}: 0.82\n",
      "  {'pos_01_0.0', 'pos_14_0.0'}: 0.77\n",
      "  {'pos_01_0.0', 'pos_05_0.0'}: 0.71\n",
      "Level 3:\n",
      "  {'pos_05_0.0', 'pos_14_0.0', 'pos_07_0.0'}: 0.70\n",
      "  {'pos_01_0.0', 'pos_06_0.0', 'pos_14_0.0'}: 0.70\n",
      "  {'pos_01_0.0', 'pos_14_0.0', 'pos_07_0.0'}: 0.77\n",
      "  {'pos_21_0.0', 'pos_14_0.0', 'pos_07_0.0'}: 0.74\n",
      "  {'pos_01_0.0', 'pos_08_0.0', 'pos_07_0.0'}: 0.73\n",
      "  {'pos_01_0.0', 'pos_06_0.0', 'pos_07_0.0'}: 0.75\n",
      "  {'pos_13_0.0', 'pos_06_0.0', 'pos_07_0.0'}: 0.71\n",
      "  {'pos_07_0.0', 'pos_06_0.0', 'pos_14_0.0'}: 0.76\n",
      "Level 4:\n",
      "  {'pos_01_0.0', 'pos_07_0.0', 'pos_06_0.0', 'pos_14_0.0'}: 0.70\n",
      "\n",
      "Association Rules:\n",
      "                 antecedents                           consequents  \\\n",
      "0               (pos_01_0.0)                          (pos_07_0.0)   \n",
      "1               (pos_07_0.0)                          (pos_01_0.0)   \n",
      "2               (pos_02_0.0)                          (pos_06_0.0)   \n",
      "3               (pos_06_0.0)                          (pos_02_0.0)   \n",
      "4               (pos_08_0.0)                          (pos_07_0.0)   \n",
      "..                       ...                                   ...   \n",
      "95  (pos_06_0.0, pos_14_0.0)              (pos_01_0.0, pos_07_0.0)   \n",
      "96              (pos_01_0.0)  (pos_06_0.0, pos_14_0.0, pos_07_0.0)   \n",
      "97              (pos_07_0.0)  (pos_01_0.0, pos_06_0.0, pos_14_0.0)   \n",
      "98              (pos_06_0.0)  (pos_01_0.0, pos_14_0.0, pos_07_0.0)   \n",
      "99              (pos_14_0.0)  (pos_01_0.0, pos_06_0.0, pos_07_0.0)   \n",
      "\n",
      "    antecedent support  consequent support   support  confidence      lift  \\\n",
      "0             0.863668            0.912250  0.825412    0.955704  1.047634   \n",
      "1             0.912250            0.863668  0.825412    0.904808  1.047634   \n",
      "2             0.772828            0.852254  0.701426    0.907609  1.064951   \n",
      "3             0.852254            0.772828  0.701426    0.823024  1.064951   \n",
      "4             0.756778            0.912250  0.726622    0.960152  1.052509   \n",
      "..                 ...                 ...       ...         ...       ...   \n",
      "95            0.763554            0.825412  0.703645    0.921540  1.116461   \n",
      "96            0.863668            0.763554  0.703645    0.814717  1.067007   \n",
      "97            0.912250            0.703645  0.703645    0.771329  1.096190   \n",
      "98            0.852254            0.769531  0.703645    0.825629  1.072899   \n",
      "99            0.844549            0.749270  0.703645    0.833161  1.111964   \n",
      "\n",
      "    representativity  leverage  conviction  zhangs_metric   jaccard  \\\n",
      "0                1.0  0.037530    1.981000       0.333510  0.868391   \n",
      "1                1.0  0.037530    1.432178       0.518157  0.868391   \n",
      "2                1.0  0.042780    1.599136       0.268474  0.759401   \n",
      "3                1.0  0.042780    1.283632       0.412802  0.759401   \n",
      "4                1.0  0.036251    2.202106       0.205120  0.771028   \n",
      "..               ...       ...         ...            ...       ...   \n",
      "95               1.0  0.073399    2.225183       0.441169  0.794792   \n",
      "96               1.0  0.044188    1.276136       0.460631  0.761870   \n",
      "97               1.0  0.061745    1.295988       1.000000  0.771329   \n",
      "98               1.0  0.047810    1.321716       0.459883  0.766382   \n",
      "99               1.0  0.070850    1.502826       0.647730  0.790459   \n",
      "\n",
      "    certainty  kulczynski  \n",
      "0    0.495205    0.930256  \n",
      "1    0.301763    0.930256  \n",
      "2    0.374662    0.865317  \n",
      "3    0.220961    0.865317  \n",
      "4    0.545889    0.878334  \n",
      "..        ...         ...  \n",
      "95   0.550599    0.887009  \n",
      "96   0.216384    0.868128  \n",
      "97   0.228388    0.885665  \n",
      "98   0.243408    0.870006  \n",
      "99   0.334587    0.886135  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Step 1: Preprocess the Connect Game Dataset\n",
    "def preprocess_connect_game(filename):\n",
    "    \"\"\"Preprocess the Connect Game dataset into transactions.\"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    # Drop the 'winner' column\n",
    "    if 'winner' in data.columns:\n",
    "        data = data.drop(columns=['winner'])\n",
    "\n",
    "    # Convert each row into a transaction\n",
    "    transactions = []\n",
    "    for _, row in data.iterrows():\n",
    "        transaction = set()\n",
    "        for col in row.index:\n",
    "            if pd.notnull(row[col]):  # Ensure valid values\n",
    "                transaction.add(f\"{col}_{row[col]}\")\n",
    "        transactions.append(transaction)\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "# Step 2: Define LCOFI Algorithm\n",
    "def generate_candidates(frequent_itemsets, size):\n",
    "    \"\"\"Generate candidate itemsets of a specific size.\"\"\"\n",
    "    return set(\n",
    "        frozenset(x) for x in itertools.combinations(set(itertools.chain(*frequent_itemsets)), size)\n",
    "    )\n",
    "\n",
    "def count_support(itemsets, transactions, min_support):\n",
    "    \"\"\"Count the support of itemsets.\"\"\"\n",
    "    support_counts = {item: 0 for item in itemsets}\n",
    "    for transaction in transactions:\n",
    "        for item in itemsets:\n",
    "            if item.issubset(transaction):\n",
    "                support_counts[item] += 1\n",
    "    return {\n",
    "        item: count for item, count in support_counts.items()\n",
    "        if count / len(transactions) >= min_support\n",
    "    }\n",
    "\n",
    "def lcofi(transactions, min_support):\n",
    "    \"\"\"LCOFI algorithm for frequent itemsets using graph representation.\"\"\"\n",
    "    # Build a bipartite graph\n",
    "    G = nx.Graph()\n",
    "    for i, transaction in enumerate(transactions):\n",
    "        for item in transaction:\n",
    "            G.add_edge(f\"Transaction_{i}\", item)\n",
    "\n",
    "    # Generate single-item frequent itemsets\n",
    "    single_items = {frozenset([node]) for node in G if G.degree[node] > 0 and not node.startswith(\"Transaction\")}\n",
    "    frequent_itemsets = count_support(single_items, transactions, min_support)\n",
    "\n",
    "    all_frequent_itemsets = [frequent_itemsets]\n",
    "\n",
    "    # Iteratively generate larger itemsets\n",
    "    k = 2\n",
    "    while frequent_itemsets:\n",
    "        candidates = generate_candidates(frequent_itemsets, k)\n",
    "        frequent_itemsets = count_support(candidates, transactions, min_support)\n",
    "        if frequent_itemsets:\n",
    "            all_frequent_itemsets.append(frequent_itemsets)\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "# Step 3: Generate Association Rules\n",
    "def generate_association_rules(frequent_itemsets, transactions, min_confidence):\n",
    "    \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
    "    # Flatten frequent itemsets\n",
    "    flat_itemsets = {}\n",
    "    for level in frequent_itemsets:\n",
    "        flat_itemsets.update(level)\n",
    "\n",
    "    # Prepare DataFrame\n",
    "    num_transactions = len(transactions)\n",
    "    data = {\n",
    "        'itemsets': list(flat_itemsets.keys()),\n",
    "        'support': [support / num_transactions for support in flat_itemsets.values()]\n",
    "    }\n",
    "    frequent_itemsets_df = pd.DataFrame(data)\n",
    "\n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets_df, metric=\"confidence\", min_threshold=min_confidence, num_itemsets=num_transactions)\n",
    "    return rules\n",
    "\n",
    "# Step 4: Load and Analyze the Dataset\n",
    "filename = \"Data/connect-game.csv\"  # Path to the Connect Game dataset\n",
    "transactions = preprocess_connect_game(filename)\n",
    "\n",
    "# Parameters\n",
    "min_support = 0.7  # Minimum support threshold\n",
    "min_confidence = 0.6  # Minimum confidence threshold\n",
    "\n",
    "# Run LCOFI Algorithm\n",
    "frequent_itemsets = lcofi(transactions, min_support)\n",
    "\n",
    "# Generate and Print Association Rules\n",
    "rules = generate_association_rules(frequent_itemsets, transactions, min_confidence)\n",
    "\n",
    "# Display Results\n",
    "print(\"Frequent Itemsets:\")\n",
    "for k, itemsets in enumerate(frequent_itemsets, start=1):\n",
    "    print(f\"Level {k}:\")\n",
    "    for itemset, support in itemsets.items():\n",
    "        print(f\"  {set(itemset)}: {support / len(transactions):.2f}\")\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a51fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
