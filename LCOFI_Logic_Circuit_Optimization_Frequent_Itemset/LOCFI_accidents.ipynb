{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4108d9cf",
   "metadata": {},
   "source": [
    "# **LCOFI Algorithm Analysis on `accidents.dat` Dataset**\n",
    "\n",
    "This document outlines the steps and methodology for applying the **LCOFI (Logic Circuit Optimization Frequent Itemset)** algorithm to the `accidents.dat` dataset. The primary objective is to discover **frequent itemsets** and generate **association rules** from the transactional data in a computationally efficient manner using graph-based techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## **Steps Involved**\n",
    "\n",
    "### **Step 1: Load the Dataset**\n",
    "\n",
    "The `chess.dat` dataset contains transactions where each line represents a set of items (represented as integers). These items correspond to attributes or characteristics of chess games. \n",
    "\n",
    "The dataset is loaded and processed into a structured format, where each line is read as a set of items (or a transaction). This transactional data will serve as the input for the LCOFI algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Represent Transactions as a Bipartite Graph**\n",
    "\n",
    "The LCOFI algorithm employs a **graph-based representation** of transactions:\n",
    "- **Nodes**: The graph consists of two types of nodes:\n",
    "  - Transaction nodes: Representing each transaction uniquely.\n",
    "  - Item nodes: Representing individual items across all transactions.\n",
    "- **Edges**: Each edge connects a transaction node to an item node if the item is present in the transaction.\n",
    "\n",
    "This bipartite representation allows efficient traversal and processing of transactions for frequent itemset mining.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Discover Frequent Itemsets**\n",
    "\n",
    "Frequent itemsets are identified using the LCOFI algorithm, which includes the following steps:\n",
    "1. **Initialize Single-Item Frequent Itemsets**:\n",
    "   - Each item is treated as a single-item candidate, and its support (occurrence frequency) is computed.\n",
    "   - Items meeting the **minimum support threshold** are retained as frequent single-itemsets.\n",
    "\n",
    "2. **Iterative Candidate Generation**:\n",
    "   - Larger candidate itemsets are generated from previously discovered frequent itemsets.\n",
    "   - For `k`-itemsets, candidates are generated by combining `k-1` frequent itemsets while ensuring all subsets are frequent (Apriori property).\n",
    "\n",
    "3. **Support Counting**:\n",
    "   - The support of each candidate itemset is computed by checking its occurrence across transactions.\n",
    "   - Candidates meeting the **minimum support threshold** are retained as frequent itemsets.\n",
    "\n",
    "4. **Graph Optimization**:\n",
    "   - The graph representation is updated dynamically to prune infrequent itemsets and reduce computational overhead.\n",
    "\n",
    "This iterative process continues until no further frequent itemsets can be generated.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Generate Association Rules**\n",
    "\n",
    "Once frequent itemsets are identified, **association rules** are generated to uncover relationships between items. These rules are evaluated using the following metrics:\n",
    "- **Support**: The proportion of transactions containing both the antecedent and consequent of the rule.\n",
    "- **Confidence**: The probability that a transaction containing the antecedent also contains the consequent.\n",
    "- **Lift**: A measure of the strength of the rule compared to random chance.\n",
    "\n",
    "Rules meeting the **minimum confidence threshold** are retained, providing valuable insights into patterns and relationships in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Output the Results**\n",
    "\n",
    "The results include:\n",
    "1. **Frequent Itemsets**:\n",
    "   - Sets of items that frequently appear together in the dataset, along with their support values.\n",
    "\n",
    "2. **Association Rules**:\n",
    "   - Logical rules derived from the frequent itemsets, showing relationships between items with metrics such as confidence and lift.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why LCOFI?**\n",
    "\n",
    "The LCOFI algorithm is chosen for its efficiency in mining frequent itemsets:\n",
    "- **Graph-Based Optimization**:\n",
    "  - By representing transactions as a bipartite graph, the algorithm can dynamically prune infrequent itemsets, reducing computational overhead.\n",
    "- **Iterative Pruning**:\n",
    "  - Candidate generation and pruning are performed iteratively, ensuring that only relevant itemsets are evaluated in subsequent steps.\n",
    "- **Scalability**:\n",
    "  - The algorithm is well-suited for large datasets like `chess.dat`, where traditional methods like Apriori may face performance bottlenecks due to multiple dataset scans.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Steps**\n",
    "\n",
    "1. **Load Transactions**:\n",
    "   - Read the dataset and store the transactions in a structured format.\n",
    "\n",
    "2. **Graph Representation**:\n",
    "   - Convert transactions into a bipartite graph with transaction and item nodes.\n",
    "\n",
    "3. **Frequent Itemset Mining**:\n",
    "   - Use the LCOFI algorithm to discover frequent itemsets based on the **minimum support** threshold.\n",
    "\n",
    "4. **Association Rule Generation**:\n",
    "   - Generate rules from frequent itemsets, filtering by the **minimum confidence** threshold.\n",
    "\n",
    "5. **Result Analysis**:\n",
    "   - Display frequent itemsets and association rules, providing insights into patterns and relationships in the chess dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications**\n",
    "\n",
    "Applying the LCOFI algorithm to the accidents.dat dataset helps uncover patterns such as:\n",
    "\n",
    "- Frequently occurring combinations of accident attributes, such as types of accidents, number of vehicles involved, and severity.\n",
    "- Relationships between factors like the number of vehicles involved, fatalities, and injuries.\n",
    "- Insights into conditions or circumstances that contribute to specific types of accidents, enabling targeted safety measures.\n",
    "\n",
    "These insights can assist policymakers, transportation authorities, and researchers in understanding accident trends, improving road safety, and formulating data-driven interventions to reduce accidents and fatalities.\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive analysis demonstrates the effectiveness of the LCOFI algorithm in mining frequent itemsets and generating association rules, enabling meaningful insights from transactional datasets like accidents.dat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf3823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "Level 1:\n",
      "  {17}: 1.00\n",
      "  {21}: 0.89\n",
      "  {16}: 0.98\n",
      "  {31}: 0.93\n",
      "  {29}: 0.88\n",
      "  {18}: 1.00\n",
      "  {12}: 1.00\n",
      "  {43}: 0.86\n",
      "Level 2:\n",
      "  {17, 43}: 0.86\n",
      "  {17, 12}: 1.00\n",
      "  {12, 21}: 0.89\n",
      "  {16, 21}: 0.87\n",
      "  {17, 21}: 0.89\n",
      "  {43, 12}: 0.86\n",
      "  {16, 17}: 0.98\n",
      "  {17, 31}: 0.93\n",
      "  {12, 31}: 0.93\n",
      "  {16, 12}: 0.98\n",
      "  {12, 29}: 0.88\n",
      "  {16, 31}: 0.92\n",
      "  {17, 18}: 1.00\n",
      "  {18, 12}: 1.00\n",
      "  {18, 43}: 0.85\n",
      "  {16, 18}: 0.97\n",
      "  {18, 21}: 0.89\n",
      "  {18, 29}: 0.88\n",
      "  {17, 29}: 0.88\n",
      "  {18, 31}: 0.93\n",
      "  {16, 29}: 0.86\n",
      "Level 3:\n",
      "  {16, 18, 31}: 0.92\n",
      "  {17, 18, 31}: 0.93\n",
      "  {16, 17, 29}: 0.86\n",
      "  {17, 12, 31}: 0.93\n",
      "  {16, 17, 12}: 0.98\n",
      "  {17, 12, 21}: 0.89\n",
      "  {16, 12, 21}: 0.87\n",
      "  {16, 17, 21}: 0.87\n",
      "  {16, 18, 29}: 0.86\n",
      "  {17, 43, 12}: 0.86\n",
      "  {16, 17, 31}: 0.92\n",
      "  {17, 18, 43}: 0.85\n",
      "  {16, 12, 29}: 0.86\n",
      "  {18, 12, 31}: 0.93\n",
      "  {17, 12, 29}: 0.88\n",
      "  {18, 12, 29}: 0.88\n",
      "  {17, 18, 21}: 0.89\n",
      "  {17, 18, 29}: 0.88\n",
      "  {18, 12, 21}: 0.89\n",
      "  {16, 18, 12}: 0.97\n",
      "  {18, 43, 12}: 0.85\n",
      "  {17, 18, 12}: 1.00\n",
      "  {16, 12, 31}: 0.92\n",
      "  {16, 17, 18}: 0.97\n",
      "  {16, 18, 21}: 0.87\n",
      "Level 4:\n",
      "  {17, 18, 12, 29}: 0.88\n",
      "  {16, 17, 18, 29}: 0.86\n",
      "  {16, 17, 12, 29}: 0.86\n",
      "  {17, 18, 12, 21}: 0.89\n",
      "  {16, 17, 12, 31}: 0.92\n",
      "  {16, 18, 12, 31}: 0.92\n",
      "  {16, 17, 18, 12}: 0.97\n",
      "  {17, 18, 12, 31}: 0.93\n",
      "  {16, 18, 12, 29}: 0.86\n",
      "  {16, 18, 12, 21}: 0.87\n",
      "  {16, 17, 18, 31}: 0.92\n",
      "  {17, 18, 43, 12}: 0.85\n",
      "  {16, 17, 18, 21}: 0.87\n",
      "  {16, 17, 12, 21}: 0.87\n",
      "Level 5:\n",
      "  {16, 17, 18, 12, 31}: 0.92\n",
      "  {16, 17, 18, 12, 29}: 0.86\n",
      "  {16, 17, 18, 21, 12}: 0.87\n",
      "\n",
      "Association Rules:\n",
      "    antecedents       consequents  antecedent support  consequent support  \\\n",
      "0          (17)              (43)            0.999906            0.856586   \n",
      "1          (43)              (17)            0.856586            0.999906   \n",
      "2          (17)              (12)            0.999906            0.998289   \n",
      "3          (12)              (17)            0.998289            0.999906   \n",
      "4          (12)              (21)            0.998289            0.889069   \n",
      "..          ...               ...                 ...                 ...   \n",
      "473        (12)  (16, 17, 18, 21)            0.998289            0.866099   \n",
      "474        (16)  (17, 18, 12, 21)            0.977162            0.885659   \n",
      "475        (17)  (16, 18, 12, 21)            0.999906            0.865760   \n",
      "476        (18)  (16, 17, 12, 21)            0.996802            0.867483   \n",
      "477        (21)  (16, 17, 18, 12)            0.889069            0.974352   \n",
      "\n",
      "      support  confidence      lift  representativity      leverage  \\\n",
      "0    0.856498    0.856578  0.999991               1.0 -7.611337e-06   \n",
      "1    0.856498    0.999897  0.999991               1.0 -7.611337e-06   \n",
      "2    0.998195    0.998289  1.000000               1.0 -1.609340e-07   \n",
      "3    0.998195    0.999906  1.000000               1.0 -1.609340e-07   \n",
      "4    0.887578    0.889099  1.000035               1.0  3.068315e-05   \n",
      "..        ...         ...       ...               ...           ...   \n",
      "473  0.865669    0.867153  1.001217               1.0  1.052579e-03   \n",
      "474  0.865669    0.885901  1.000274               1.0  2.371714e-04   \n",
      "475  0.865669    0.865751  0.999989               1.0 -9.687916e-06   \n",
      "476  0.865669    0.868447  1.001111               1.0  9.607229e-04   \n",
      "477  0.865669    0.973681  0.999312               1.0 -5.963763e-04   \n",
      "\n",
      "     conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
      "0      0.999947      -0.086316  0.856503  -0.000053    0.928238  \n",
      "1      0.913692      -0.000062  0.856503  -0.094461    0.928238  \n",
      "2      0.999906      -0.001711  0.998195  -0.000094    0.999097  \n",
      "3      0.998289      -0.000094  0.998195  -0.001714    0.999097  \n",
      "4      1.000277       0.020206  0.887774   0.000277    0.943711  \n",
      "..          ...            ...       ...        ...         ...  \n",
      "473    1.007937       0.710709  0.866780   0.007874    0.933329  \n",
      "474    1.002127       0.011997  0.868142   0.002123    0.931666  \n",
      "475    0.999928      -0.106322  0.865672  -0.000072    0.932823  \n",
      "476    1.007326       0.347000  0.866870   0.007273    0.933178  \n",
      "477    0.974513      -0.006172  0.867620  -0.026154    0.931069  \n",
      "\n",
      "[478 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Step 1: Load and Preprocess the Dataset\n",
    "def load_chess_dat(filename):\n",
    "    \"\"\"Load and preprocess the .dat file into transactions.\"\"\"\n",
    "    transactions = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split each line into items and convert to a set\n",
    "            transaction = set(map(int, line.strip().split()))\n",
    "            transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Step 2: LCOFI Algorithm Functions\n",
    "def generate_candidates(frequent_itemsets, size):\n",
    "    \"\"\"Generate candidate itemsets of a specific size.\"\"\"\n",
    "    return set(\n",
    "        frozenset(x) for x in itertools.combinations(set(itertools.chain(*frequent_itemsets)), size)\n",
    "    )\n",
    "\n",
    "def count_support(itemsets, transactions, min_support):\n",
    "    \"\"\"Count the support of itemsets.\"\"\"\n",
    "    support_counts = {item: 0 for item in itemsets}\n",
    "    for transaction in transactions:\n",
    "        for item in itemsets:\n",
    "            if item.issubset(transaction):\n",
    "                support_counts[item] += 1\n",
    "    return {\n",
    "        item: count for item, count in support_counts.items()\n",
    "        if count / len(transactions) >= min_support\n",
    "    }\n",
    "\n",
    "def lcofi(transactions, min_support):\n",
    "    \"\"\"LCOFI algorithm for frequent itemsets using graph representation.\"\"\"\n",
    "    # Build a bipartite graph\n",
    "    G = nx.Graph()\n",
    "    for i, transaction in enumerate(transactions):\n",
    "        transaction_node = f\"Transaction_{i}\"  # Transaction nodes are strings\n",
    "        for item in transaction:\n",
    "            G.add_edge(transaction_node, item)\n",
    "\n",
    "    # Generate single-item frequent itemsets\n",
    "    single_items = {frozenset([node]) for node in G.nodes if isinstance(node, int)}\n",
    "    frequent_itemsets = count_support(single_items, transactions, min_support)\n",
    "\n",
    "    all_frequent_itemsets = [frequent_itemsets]\n",
    "\n",
    "    # Iteratively generate larger itemsets\n",
    "    k = 2\n",
    "    while frequent_itemsets:\n",
    "        candidates = generate_candidates(frequent_itemsets, k)\n",
    "        frequent_itemsets = count_support(candidates, transactions, min_support)\n",
    "        if frequent_itemsets:\n",
    "            all_frequent_itemsets.append(frequent_itemsets)\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "# Step 3: Generate Association Rules\n",
    "def generate_association_rules(frequent_itemsets, transactions, min_confidence):\n",
    "    \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
    "    # Flatten frequent itemsets\n",
    "    flat_itemsets = {}\n",
    "    for level in frequent_itemsets:\n",
    "        flat_itemsets.update(level)\n",
    "\n",
    "    # Prepare DataFrame\n",
    "    num_transactions = len(transactions)\n",
    "    data = {\n",
    "        'itemsets': list(flat_itemsets.keys()),\n",
    "        'support': [support / num_transactions for support in flat_itemsets.values()]\n",
    "    }\n",
    "    frequent_itemsets_df = pd.DataFrame(data)\n",
    "\n",
    "    # Generate association rules\n",
    "    rules = association_rules(frequent_itemsets_df, metric=\"confidence\", min_threshold=min_confidence, num_itemsets=num_transactions)\n",
    "    return rules\n",
    "\n",
    "# Step 4: Apply LCOFI on Chess Dataset\n",
    "filename = \"Data/accidents.dat\"  # Replace with the actual path to chess.dat\n",
    "transactions = load_chess_dat(filename)\n",
    "\n",
    "# Parameters\n",
    "min_support = 0.85  # Minimum support threshold\n",
    "min_confidence = 0.6  # Minimum confidence threshold\n",
    "\n",
    "# Run LCOFI Algorithm\n",
    "frequent_itemsets = lcofi(transactions, min_support)\n",
    "\n",
    "# Generate and Print Association Rules\n",
    "rules = generate_association_rules(frequent_itemsets, transactions, min_confidence)\n",
    "\n",
    "# Display Results\n",
    "print(\"Frequent Itemsets:\")\n",
    "for k, itemsets in enumerate(frequent_itemsets, start=1):\n",
    "    print(f\"Level {k}:\")\n",
    "    for itemset, support in itemsets.items():\n",
    "        print(f\"  {set(itemset)}: {support / len(transactions):.2f}\")\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62f23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
