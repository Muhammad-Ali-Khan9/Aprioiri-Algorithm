{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4108d9cf",
   "metadata": {},
   "source": [
    "# Apriori Algorithm Analysis on `accidents.dat` Dataset\n",
    "\n",
    "This document provides an overview of the steps involved in applying the **Apriori algorithm** to the `accidents.dat` dataset. The goal is to discover **frequent itemsets** and generate **association rules** from the given transaction data.\n",
    "\n",
    "## Steps Involved\n",
    "\n",
    "1. **Load the Dataset**\n",
    "2. **Convert Transactions to One-Hot Encoded Format**\n",
    "3. **Apply the Apriori Algorithm**\n",
    "4. **Generate Association Rules**\n",
    "5. **Output the Results**\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Load the Dataset\n",
    "\n",
    "The first step in the process is to **load the dataset** (`accidents.dat` file). Each line in this file represents a transaction where items (represented as integers) are involved. The objective is to read these transactions and store them in a structured format (a list of transactions), which will later be processed by the Apriori algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Convert Transactions to One-Hot Encoded Format\n",
    "\n",
    "Once the transactions are loaded, the next task is to **convert them into a one-hot encoded format**. This transformation will represent each transaction as a binary vector where:\n",
    "- Each vector element corresponds to an item in the dataset (based on all unique items).\n",
    "- A value of `1` indicates that the item is present in the transaction.\n",
    "- A value of `0` indicates that the item is absent.\n",
    "\n",
    "This step is essential for the Apriori algorithm, as it operates on a binary matrix where each row is a transaction, and each column is an item.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Apply the Apriori Algorithm\n",
    "\n",
    "With the data in the one-hot encoded format, we can now apply the **Apriori algorithm**. The Apriori algorithm finds **frequent itemsets**, which are groups of items that appear together in the dataset more frequently than a specified threshold, called **minimum support**.\n",
    "\n",
    "- The algorithm starts by finding individual frequent items.\n",
    "- It then iterates to find pairs, triples, and larger itemsets, checking whether they meet the **minimum support** threshold.\n",
    "- The algorithm is iterative and reduces the candidate itemsets at each step based on the support count.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Generate Association Rules\n",
    "\n",
    "After the frequent itemsets are identified, the next step is to **generate association rules** from these itemsets. Association rules provide insights into how the occurrence of one item (or set of items) in a transaction can imply the occurrence of other items.\n",
    "\n",
    "- **Confidence** is used as the metric for generating these rules. It is defined as the probability that a transaction containing the antecedent (left-hand side) also contains the consequent (right-hand side) of the rule.\n",
    "- Rules are generated from itemsets that meet the **minimum confidence** threshold, which ensures that the rules are statistically significant.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Output the Results\n",
    "\n",
    "Finally, the results are displayed, including:\n",
    "- **Frequent itemsets**: The sets of items that frequently appear together in the dataset.\n",
    "- **Association rules**: The rules that show relationships between different items in the dataset.\n",
    "\n",
    "These results can be analyzed to understand patterns or co-occurrences in accident-related data, providing valuable insights for further research or decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Steps\n",
    "\n",
    "1. **Load Transactions**: Read the dataset and store the transactions in a structured format.\n",
    "2. **One-Hot Encoding**: Convert transactions into a one-hot encoded format where each column represents an item.\n",
    "3. **Apriori Algorithm**: Apply the Apriori algorithm to find frequent itemsets based on the **minimum support** threshold.\n",
    "4. **Association Rules**: Generate association rules using the frequent itemsets, filtered by the **minimum confidence** threshold.\n",
    "5. **Display Results**: Show the frequent itemsets and association rules found during the analysis.\n",
    "\n",
    "---\n",
    "\n",
    "This approach allows us to apply the Apriori algorithm on the accident dataset to uncover hidden relationships between different attributes of accidents, helping in predictive analysis and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf3823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "      support                  itemsets\n",
      "0    0.998289                      (12)\n",
      "1    0.802268                      (15)\n",
      "2    0.977162                      (16)\n",
      "3    0.999906                      (17)\n",
      "4    0.996802                      (18)\n",
      "..        ...                       ...\n",
      "144  0.800199      (43, 12, 17, 18, 31)\n",
      "145  0.817725      (16, 17, 18, 21, 31)\n",
      "146  0.804247      (16, 17, 18, 29, 31)\n",
      "147  0.817369  (12, 16, 17, 18, 21, 31)\n",
      "148  0.803903  (12, 16, 17, 18, 29, 31)\n",
      "\n",
      "[149 rows x 2 columns]\n",
      "\n",
      "Association Rules:\n",
      "     antecedents           consequents  antecedent support  \\\n",
      "0           (12)                  (15)            0.998289   \n",
      "1           (15)                  (12)            0.802268   \n",
      "2           (16)                  (12)            0.977162   \n",
      "3           (12)                  (16)            0.998289   \n",
      "4           (17)                  (12)            0.999906   \n",
      "...          ...                   ...                 ...   \n",
      "1427        (16)  (12, 17, 18, 29, 31)            0.977162   \n",
      "1428        (17)  (12, 16, 18, 29, 31)            0.999906   \n",
      "1429        (18)  (12, 16, 17, 29, 31)            0.996802   \n",
      "1430        (29)  (12, 16, 17, 18, 31)            0.879500   \n",
      "1431        (31)  (12, 16, 17, 18, 29)            0.934832   \n",
      "\n",
      "      consequent support   support  confidence      lift  representativity  \\\n",
      "0               0.802268  0.800743    0.802115  0.999809               1.0   \n",
      "1               0.998289  0.800743    0.998098  0.999809               1.0   \n",
      "2               0.998289  0.976430    0.999251  1.000963               1.0   \n",
      "3               0.977162  0.976430    0.978104  1.000963               1.0   \n",
      "4               0.998289  0.998195    0.998289  1.000000               1.0   \n",
      "...                  ...       ...         ...       ...               ...   \n",
      "1427            0.819247  0.803903    0.822691  1.004203               1.0   \n",
      "1428            0.803979  0.803903    0.803978  0.999999               1.0   \n",
      "1429            0.805487  0.803903    0.806482  1.001235               1.0   \n",
      "1430            0.915422  0.803903    0.914045  0.998496               1.0   \n",
      "1431            0.855634  0.803903    0.859943  1.005037               1.0   \n",
      "\n",
      "          leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
      "0    -1.530938e-04    0.999225      -0.100519  0.800891  -0.000776    0.900107  \n",
      "1    -1.530938e-04    0.899653      -0.000966  0.800891  -0.111539    0.900107  \n",
      "2     9.398131e-04    2.283970       0.042145  0.977387   0.562166    0.988677  \n",
      "3     9.398131e-04    1.042995       0.562587  0.977387   0.041222    0.988677  \n",
      "4    -1.609340e-07    0.999906      -0.001711  0.998195  -0.000094    0.999097  \n",
      "...            ...         ...            ...       ...        ...         ...  \n",
      "1427  3.365048e-03    1.019422       0.183289  0.809972   0.019052    0.901980  \n",
      "1428 -8.015418e-07    0.999996      -0.010488  0.803917  -0.000004    0.901942  \n",
      "1429  9.917306e-04    1.005141       0.385722  0.805202   0.005115    0.902257  \n",
      "1430 -1.211176e-03    0.983979      -0.012349  0.811187  -0.016282    0.896111  \n",
      "1431  4.028834e-03    1.030771       0.076903  0.814852   0.029852    0.899742  \n",
      "\n",
      "[1432 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 1: Load the Data\n",
    "def load_dat_file(filename):\n",
    "    \"\"\"Read the .dat file and convert it to a list of transactions.\"\"\"\n",
    "    transactions = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            transaction = list(map(int, line.strip().split()))\n",
    "            transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Step 2: Convert Transactions to a One-Hot Encoded DataFrame\n",
    "def transactions_to_dataframe(transactions):\n",
    "    \"\"\"Convert the list of transactions to a one-hot encoded DataFrame.\"\"\"\n",
    "    # Get all unique items\n",
    "    unique_items = set(item for transaction in transactions for item in transaction)\n",
    "    \n",
    "    # Create a DataFrame with columns for each unique item\n",
    "    encoded_df = pd.DataFrame(0, index=range(len(transactions)), columns=sorted(unique_items))\n",
    "    for i, transaction in enumerate(transactions):\n",
    "        for item in transaction:\n",
    "            encoded_df.loc[i, item] = 1\n",
    "    \n",
    "    # Convert to boolean type for compatibility with mlxtend\n",
    "    return encoded_df.astype(bool)\n",
    "\n",
    "# Step 3: Apply Apriori and Generate Association Rules\n",
    "def apply_apriori(data, min_support, min_confidence):\n",
    "    \"\"\"Apply the Apriori algorithm and generate association rules.\"\"\"\n",
    "    # Apply Apriori\n",
    "    frequent_itemsets = apriori(data, min_support=min_support, use_colnames=True)\n",
    "    \n",
    "    # Generate Association Rules\n",
    "    num_itemsets = len(data)  # Calculate the total number of transactions\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence, num_itemsets=num_itemsets)\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "# Main Execution\n",
    "filename = \"Data/accidents.dat\"  # Replace with your actual .dat file path\n",
    "transactions = load_dat_file(filename)\n",
    "encoded_data = transactions_to_dataframe(transactions)\n",
    "\n",
    "# Parameters\n",
    "min_support = 0.8  # Minimum support threshold\n",
    "min_confidence = 0.6  # Minimum confidence threshold\n",
    "\n",
    "# Apply Apriori\n",
    "frequent_itemsets, rules = apply_apriori(encoded_data, min_support, min_confidence)\n",
    "\n",
    "# Display Results\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a51fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
